{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from rbflayer import RBFLayer, InitCentersRandom\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://benalexkeen.com/feature-scaling-with-scikit-learn/\n",
    "# https://github.com/PetraVidnerova/rbf_keras/blob/master/rbflayer.py\n",
    "\n",
    "def load_dataset(is_normalize=True):\n",
    "    global dataset, labels\n",
    "    with open('OnlineNewsPopularity.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        next(reader) # skip the first row\n",
    "        temp = []\n",
    "        for idx, row in enumerate(reader):\n",
    "            temp.append(row[1:])\n",
    "        if is_normalize:\n",
    "            # it is important which type of normalization method you use\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            temp = scaler.fit_transform(temp)\n",
    "            print('Data is normalize')\n",
    "        else:\n",
    "            print('No normalize apply')\n",
    "        for idx, row in enumerate(temp):\n",
    "            slice_row = np.append(row[1:45], row[46:])\n",
    "            labels.append(row[45])\n",
    "            dataset.append(slice_row)\n",
    "    return np.array(dataset), np.array(labels).reshape(len(labels), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is normalize\n",
      "Dataset shape: (39644, 58), Labels: (39644, 1)\n",
      "x_train: (31715, 58), y_train: (31715, 1)\n",
      "x_test: (7929, 58), y_test: (7929, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset \n",
    "dataset = []\n",
    "labels = []\n",
    "# ori_dataset, ori_labels = load_dataset(is_normalize=False) \n",
    "dataset, labels = load_dataset(is_normalize=True)\n",
    "print('Dataset shape: {0}, Labels: {1}'.format(dataset.shape, labels.shape))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "print(\"x_train: {0}, y_train: {1}\".format(x_train.shape, y_train.shape))\n",
    "print(\"x_test: {0}, y_test: {1}\".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.19047619e-01 7.41090394e-02 7.27116601e-04 9.59692896e-04\n",
      " 9.92436817e-04 4.60526316e-02 8.62068966e-03 7.81250000e-03\n",
      " 0.00000000e+00 6.33058894e-01 6.66666667e-01 0.00000000e+00\n",
      " 0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 2.26876676e-03 7.03994636e-03\n",
      " 9.01221392e-03 1.00000000e+00 3.95842862e-01 4.23732082e-01\n",
      " 2.12057641e-02 8.72852433e-02 8.73947587e-04 8.73947587e-04\n",
      " 8.73947587e-04 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 5.08539436e-01 3.08572373e-02 3.13792118e-02\n",
      " 4.77560112e-01 3.09215853e-02 3.16003500e-01 3.93611889e-01\n",
      " 6.02736494e-02 6.95652174e-01 3.04347826e-01 2.75970644e-01\n",
      " 3.33333333e-02 8.00000000e-01 7.40816327e-01 5.00000000e-01\n",
      " 9.00000000e-01 4.58333333e-01 6.66666667e-01 8.33333333e-02\n",
      " 3.33333333e-01 1.90905005e-02]\n",
      "[[0.16385663]\n",
      " [0.29876792]\n",
      " [0.31690821]\n",
      " [0.44456492]\n",
      " [0.26143791]\n",
      " [0.44870041]\n",
      " [0.28425956]\n",
      " [0.23037752]\n",
      " [0.35729847]\n",
      " [0.35265824]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rbf_layer_3 (RBFLayer)       (None, 1)                 59        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 61\n",
      "Trainable params: 61\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build models\n",
    "model = Sequential()\n",
    "rbflayer = RBFLayer(1,\n",
    "                    initializer=InitCentersRandom(x_train), \n",
    "                    betas=2.0,\n",
    "                    input_shape=(58,))\n",
    "model.add(rbflayer)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=RMSprop(lr=0.001))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded: best_weights.h5\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/50\n",
      "31715/31715 [==============================] - 2s 76us/step - loss: 0.0067 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00683, saving model to best_weights.h5\n",
      "Epoch 2/50\n",
      "31715/31715 [==============================] - 3s 82us/step - loss: 0.0067 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00683 to 0.00683, saving model to best_weights.h5\n",
      "Epoch 3/50\n",
      "31715/31715 [==============================] - 3s 80us/step - loss: 0.0067 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00683 to 0.00683, saving model to best_weights.h5\n",
      "Epoch 4/50\n",
      "31715/31715 [==============================] - 3s 80us/step - loss: 0.0067 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00683 to 0.00683, saving model to best_weights.h5\n",
      "Epoch 5/50\n",
      "31715/31715 [==============================] - 3s 81us/step - loss: 0.0067 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00683 to 0.00683, saving model to best_weights.h5\n",
      "Epoch 6/50\n",
      "31715/31715 [==============================] - 3s 81us/step - loss: 0.0067 - val_loss: 0.0068\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00683 to 0.00683, saving model to best_weights.h5\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19605ac64a8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training with callbacks\n",
    "saved_weights_name = 'best_weights.h5'\n",
    "\n",
    "# load weights if any\n",
    "if saved_weights_name:\n",
    "    model.load_weights(saved_weights_name)\n",
    "    print(\"Weights loaded: {0}\".format(saved_weights_name))\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=0.0001,\n",
    "                           patience=5,\n",
    "                           mode='min',\n",
    "                           verbose=1)\n",
    "\n",
    "checkpoint = ModelCheckpoint(saved_weights_name,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='min',\n",
    "                             period=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=1,\n",
    "                              min_lr=0.00001,\n",
    "                              verbose=1)\n",
    "    \n",
    "model.fit(x_train, y_train,\n",
    "          epochs=50,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[early_stop, checkpoint, reduce_lr],\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing results\n",
    "plot([1,2,3], label=\"predict\")\n",
    "plot([3,2,1], label=\"actual\")\n",
    "legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
