{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os.path\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from rbflayer import RBFLayer, InitCentersRandom\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "from rbflayer import RBFLayer, InitCentersRandom\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading of dataset \n",
    "dataset = []\n",
    "labels = []\n",
    "headers = []\n",
    "\n",
    "def load_dataset_raw():\n",
    "    global dataset, labels, headers\n",
    "    with open('OnlineNewsPopularity.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        headers = next(reader) # the first row\n",
    "        headers = headers[1:]\n",
    "        headers = [x.strip(' ') for x in headers]    \n",
    "            \n",
    "        temp_data = []\n",
    "        temp_label = []\n",
    "        for idx, row in enumerate(reader):\n",
    "            temp_data.append(row[1:])\n",
    "            temp_label.append(row[-1])\n",
    "\n",
    "        # convert elements to float        \n",
    "        for idx, row in enumerate(temp_data):        \n",
    "            dataset.append([float(i) for i in row[:]])\n",
    "        for idx, row in enumerate(temp_label):\n",
    "            labels.append(float(row))   \n",
    "        \n",
    "    return np.array(dataset), np.array(labels).reshape(-1, 1)\n",
    "\n",
    "def load_dataset(is_normalize=True, is_remove_outlier=True, select_top=50):\n",
    "    \"\"\"\n",
    "    is_normalize: To normalize or keep orignal form\n",
    "    is_remove_outlier: To remove outlier \n",
    "    select_top: select top important features using PCA (Principal component analysis)\n",
    "    \"\"\"\n",
    "    global dataset, labels, headers\n",
    "    dataset, labels = load_dataset_raw()\n",
    "    print('Original dataset shape: {0}, Labels: {1}'.format(dataset.shape, labels.shape))\n",
    "    \n",
    "    fields_to_avoid = [0, 18, 20, 24, 38, 39, 40, 41, 42]\n",
    "    weekday = [30, 31, 32, 33, 34, 35, 36, 37]\n",
    "    data_channel = [12, 13, 14, 15, 16, 17]\n",
    "    fields_to_avoid = fields_to_avoid + weekday + data_channel\n",
    "    fields_to_avoid = sorted(fields_to_avoid)\n",
    "\n",
    "    # adjust headers\n",
    "    sub_headers = []\n",
    "    for idx, h in enumerate(headers):\n",
    "        if idx not in fields_to_avoid:\n",
    "            sub_headers.append(h)\n",
    "    sub_headers.insert(len(sub_headers) - 1, 'weekday')\n",
    "    sub_headers.insert(len(sub_headers) - 1, 'data_channel')\n",
    "    headers = sub_headers\n",
    "    \n",
    "    # filter columns and categorical fields\n",
    "    new_dataset = []        \n",
    "    for idx, row in enumerate(dataset):\n",
    "        slice_row = [i for num,i in enumerate(row) if num not in fields_to_avoid]\n",
    "        insert_idx = len(slice_row) - 1\n",
    "\n",
    "        for col_idx in range(len(row)):\n",
    "            # convert week day to categorical \n",
    "            if col_idx in weekday and row[col_idx] == 1:\n",
    "                if col_idx == 30:\n",
    "                    slice_row.insert(insert_idx, 1)\n",
    "                elif col_idx == 31:\n",
    "                    slice_row.insert(insert_idx, 2)\n",
    "                elif col_idx == 32:\n",
    "                    slice_row.insert(insert_idx, 3)\n",
    "                elif col_idx == 33:\n",
    "                    slice_row.insert(insert_idx, 4)\n",
    "                elif col_idx == 34:\n",
    "                    slice_row.insert(insert_idx, 5)\n",
    "                elif col_idx == 35:\n",
    "                    slice_row.insert(insert_idx, 6)\n",
    "                elif col_idx == 36:\n",
    "                    slice_row.insert(insert_idx, 7)\n",
    "\n",
    "            # convert data channel to categorical \n",
    "            if col_idx in data_channel and row[col_idx] == 1:\n",
    "                if col_idx == 12:\n",
    "                    slice_row.insert(insert_idx, 1)\n",
    "                elif col_idx == 13:\n",
    "                    slice_row.insert(insert_idx, 2)\n",
    "                elif col_idx == 14:\n",
    "                    slice_row.insert(insert_idx, 3)\n",
    "                elif col_idx == 15:\n",
    "                    slice_row.insert(insert_idx, 4)\n",
    "                elif col_idx == 16:\n",
    "                    slice_row.insert(insert_idx, 5)\n",
    "                elif col_idx == 17:\n",
    "                    slice_row.insert(insert_idx, 6)\n",
    "\n",
    "        # handle missing data\n",
    "        if len(slice_row) == 38:\n",
    "            slice_row.append(0)\n",
    "        new_dataset.append(slice_row)         \n",
    "    \n",
    "    # copy filter new_dataset to dataset \n",
    "    dataset = new_dataset\n",
    "    \n",
    "    # normalizing in dataset\n",
    "    if is_normalize:\n",
    "        # it is important which type of normalization method you use\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        dataset = scaler.fit_transform(dataset)\n",
    "        # last item uses MinMaxScaler\n",
    "        labels = np.array(labels).reshape(-1, 1)\n",
    "        scaler = preprocessing.QuantileTransformer()\n",
    "        labels = scaler.fit_transform(labels)\n",
    "        print('Data is normalize')\n",
    "    else:\n",
    "        print('No normalize apply')        \n",
    "\n",
    "    # remove outlier in dataset\n",
    "    if is_remove_outlier:\n",
    "        dataset, labels = reject_outliers(dataset, labels)    \n",
    "\n",
    "    # applying PCA in dataset\n",
    "    if 0 < select_top < len(dataset[0]):\n",
    "        top_features = pca_important_features(dataset, headers, top=select_top)\n",
    "        feature_keys = [f[1] for f in top_features]\n",
    "        df = pd.DataFrame(dataset, columns=headers)\n",
    "        top_dataset = []\n",
    "        for name in feature_keys:\n",
    "            top_dataset.append(df.loc[:, name])\n",
    "        dataset = np.transpose(top_dataset)\n",
    "        \n",
    "    return np.array(dataset), np.array(labels).reshape(-1, 1)\n",
    "\n",
    "def pca_important_features(dataset, headers, top=10):\n",
    "    scaleFeatures = True\n",
    "    pca = PCA(n_components=top)\n",
    "    projected  = pca.fit_transform(dataset)\n",
    "    top_features = extract_features(projected, pca.components_, headers, top)\n",
    "    return top_features\n",
    "    \n",
    "def extract_features(transformed_features, components_, columns, top):\n",
    "    num_columns = len(columns)\n",
    "\n",
    "    # This funtion will project your *original* feature (columns)\n",
    "    # onto your principal component feature-space, so that you can\n",
    "    # visualize how \"important\" each one was in the\n",
    "    # multi-dimensional scaling\n",
    "\n",
    "    # Scale the principal components by the max value in\n",
    "    # the transformed set belonging to that component\n",
    "    xvector = components_[0] * max(transformed_features[:,0])\n",
    "    yvector = components_[1] * max(transformed_features[:,1])\n",
    "\n",
    "    # Sort each column by it's length. These are your *original*\n",
    "    # columns, not the principal components.\n",
    "    important_features = { columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(num_columns) }\n",
    "    important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True)\n",
    "    print(\"Features by top {0} importance:\".format(top))\n",
    "    for feature in important_features[:top]:\n",
    "        print(feature)\n",
    "    print()\n",
    "    return important_features[:top]\n",
    "        \n",
    "def reject_outliers(dataset, labels, m=50):        \n",
    "    \"\"\"\n",
    "    Higher the value 'm' more outliers\n",
    "    \"\"\"\n",
    "    d = np.abs(labels - np.median(labels))\n",
    "    mdev = np.median(d)\n",
    "    s = d/mdev if mdev else 0.\n",
    "        \n",
    "    # remove outliers in dataset and labels\n",
    "    sub_dataset = []\n",
    "    sub_labels = []\n",
    "    outlier_labels = []\n",
    "    for idx, item in enumerate(s):        \n",
    "        if item < m:\n",
    "            sub_labels.append(labels[idx])\n",
    "            sub_dataset.append(dataset[idx])        \n",
    "        else:\n",
    "            outlier_labels.append(labels[idx])\n",
    "    print('Outlier removed: {0}'.format(len(outlier_labels)))\n",
    "    return sub_dataset, sub_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded: best_features_extraction_rbf.h5\n",
      "Weights loaded: best_weights_rbf.h5\n",
      "Original dataset shape: (39644, 60), Labels: (39644, 1)\n",
      "Data is normalize\n",
      "Outlier removed: 0\n",
      "Dataset shape: (39644, 39), Labels: (39644, 1)\n",
      "x_train: (31715, 39), y_train: (31715, 1)\n",
      "x_test: (7929, 39), y_test: (7929, 1)\n",
      "Train on 3171 samples, validate on 7929 samples\n",
      "Epoch 1/50\n",
      "3171/3171 [==============================] - 1s 243us/step - loss: 0.0443 - mean_squared_error: 0.0363 - val_loss: 0.0237 - val_mean_squared_error: 0.0158\n",
      "Epoch 2/50\n",
      "3171/3171 [==============================] - 0s 155us/step - loss: 0.0447 - mean_squared_error: 0.0368 - val_loss: 0.0217 - val_mean_squared_error: 0.0138\n",
      "Epoch 3/50\n",
      "3171/3171 [==============================] - 0s 152us/step - loss: 0.0449 - mean_squared_error: 0.0369 - val_loss: 0.0229 - val_mean_squared_error: 0.0149\n",
      "Epoch 4/50\n",
      "3171/3171 [==============================] - 0s 157us/step - loss: 0.0433 - mean_squared_error: 0.0353 - val_loss: 0.0229 - val_mean_squared_error: 0.0150\n",
      "Epoch 5/50\n",
      "3171/3171 [==============================] - 0s 151us/step - loss: 0.0425 - mean_squared_error: 0.0345 - val_loss: 0.0215 - val_mean_squared_error: 0.0135\n",
      "Epoch 6/50\n",
      "3171/3171 [==============================] - 0s 151us/step - loss: 0.0434 - mean_squared_error: 0.0354 - val_loss: 0.0213 - val_mean_squared_error: 0.0134\n",
      "Epoch 7/50\n",
      "3171/3171 [==============================] - 0s 151us/step - loss: 0.0417 - mean_squared_error: 0.0338 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 8/50\n",
      "3171/3171 [==============================] - 0s 148us/step - loss: 0.0420 - mean_squared_error: 0.0340 - val_loss: 0.0215 - val_mean_squared_error: 0.0136\n",
      "Epoch 9/50\n",
      "3171/3171 [==============================] - 0s 146us/step - loss: 0.0427 - mean_squared_error: 0.0348 - val_loss: 0.0236 - val_mean_squared_error: 0.0157\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 10/50\n",
      "3171/3171 [==============================] - 1s 162us/step - loss: 0.0434 - mean_squared_error: 0.0355 - val_loss: 0.0219 - val_mean_squared_error: 0.0140\n",
      "Epoch 11/50\n",
      "3171/3171 [==============================] - 0s 155us/step - loss: 0.0432 - mean_squared_error: 0.0352 - val_loss: 0.0223 - val_mean_squared_error: 0.0143\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 12/50\n",
      "3171/3171 [==============================] - 1s 170us/step - loss: 0.0425 - mean_squared_error: 0.0345 - val_loss: 0.0218 - val_mean_squared_error: 0.0139\n",
      "Epoch 00012: early stopping\n",
      "Weights loaded: best_features_extraction_rbf.h5\n",
      "Weights loaded: best_weights_rbf.h5\n",
      "Original dataset shape: (39644, 60), Labels: (39644, 1)\n",
      "Data is normalize\n",
      "Outlier removed: 0\n",
      "Dataset shape: (39644, 39), Labels: (39644, 1)\n",
      "x_train: (31715, 39), y_train: (31715, 1)\n",
      "x_test: (7929, 39), y_test: (7929, 1)\n",
      "Train on 6343 samples, validate on 7929 samples\n",
      "Epoch 1/50\n",
      "6343/6343 [==============================] - 1s 152us/step - loss: 0.0404 - mean_squared_error: 0.0325 - val_loss: 0.0236 - val_mean_squared_error: 0.0157\n",
      "Epoch 2/50\n",
      "6343/6343 [==============================] - 1s 120us/step - loss: 0.0401 - mean_squared_error: 0.0322 - val_loss: 0.0241 - val_mean_squared_error: 0.0162\n",
      "Epoch 3/50\n",
      "6343/6343 [==============================] - 1s 122us/step - loss: 0.0395 - mean_squared_error: 0.0316 - val_loss: 0.0245 - val_mean_squared_error: 0.0165\n",
      "Epoch 4/50\n",
      "6343/6343 [==============================] - 1s 124us/step - loss: 0.0400 - mean_squared_error: 0.0321 - val_loss: 0.0216 - val_mean_squared_error: 0.0136\n",
      "Epoch 5/50\n",
      "6343/6343 [==============================] - 1s 128us/step - loss: 0.0407 - mean_squared_error: 0.0328 - val_loss: 0.0223 - val_mean_squared_error: 0.0144\n",
      "Epoch 6/50\n",
      "6343/6343 [==============================] - 1s 131us/step - loss: 0.0392 - mean_squared_error: 0.0313 - val_loss: 0.0232 - val_mean_squared_error: 0.0152\n",
      "Epoch 7/50\n",
      "6343/6343 [==============================] - 1s 133us/step - loss: 0.0389 - mean_squared_error: 0.0310 - val_loss: 0.0235 - val_mean_squared_error: 0.0155\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/50\n",
      "6343/6343 [==============================] - 1s 128us/step - loss: 0.0391 - mean_squared_error: 0.0311 - val_loss: 0.0242 - val_mean_squared_error: 0.0163\n",
      "Epoch 9/50\n",
      "6343/6343 [==============================] - 1s 125us/step - loss: 0.0380 - mean_squared_error: 0.0300 - val_loss: 0.0244 - val_mean_squared_error: 0.0164\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/50\n",
      "6343/6343 [==============================] - 1s 133us/step - loss: 0.0382 - mean_squared_error: 0.0302 - val_loss: 0.0239 - val_mean_squared_error: 0.0159\n",
      "Epoch 00010: early stopping\n",
      "Weights loaded: best_features_extraction_rbf.h5\n",
      "Weights loaded: best_weights_rbf.h5\n",
      "Original dataset shape: (39644, 60), Labels: (39644, 1)\n",
      "Data is normalize\n",
      "Outlier removed: 0\n",
      "Dataset shape: (39644, 39), Labels: (39644, 1)\n",
      "x_train: (31715, 39), y_train: (31715, 1)\n",
      "x_test: (7929, 39), y_test: (7929, 1)\n",
      "Train on 9514 samples, validate on 7929 samples\n",
      "Epoch 1/50\n",
      "9514/9514 [==============================] - 1s 134us/step - loss: 0.0414 - mean_squared_error: 0.0334 - val_loss: 0.0235 - val_mean_squared_error: 0.0156\n",
      "Epoch 2/50\n",
      "9514/9514 [==============================] - 1s 107us/step - loss: 0.0417 - mean_squared_error: 0.0337 - val_loss: 0.0225 - val_mean_squared_error: 0.0146\n",
      "Epoch 3/50\n",
      "9514/9514 [==============================] - 1s 109us/step - loss: 0.0416 - mean_squared_error: 0.0337 - val_loss: 0.0214 - val_mean_squared_error: 0.0134\n",
      "Epoch 4/50\n",
      "9514/9514 [==============================] - 1s 111us/step - loss: 0.0415 - mean_squared_error: 0.0336 - val_loss: 0.0213 - val_mean_squared_error: 0.0134\n",
      "Epoch 5/50\n",
      "9514/9514 [==============================] - 1s 107us/step - loss: 0.0409 - mean_squared_error: 0.0329 - val_loss: 0.0215 - val_mean_squared_error: 0.0136\n",
      "Epoch 6/50\n",
      "9514/9514 [==============================] - 1s 103us/step - loss: 0.0428 - mean_squared_error: 0.0348 - val_loss: 0.0252 - val_mean_squared_error: 0.0172\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/50\n",
      "9514/9514 [==============================] - 1s 107us/step - loss: 0.0421 - mean_squared_error: 0.0341 - val_loss: 0.0227 - val_mean_squared_error: 0.0148\n",
      "Epoch 8/50\n",
      "9514/9514 [==============================] - 1s 103us/step - loss: 0.0413 - mean_squared_error: 0.0334 - val_loss: 0.0222 - val_mean_squared_error: 0.0142\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 9/50\n",
      "9514/9514 [==============================] - 1s 102us/step - loss: 0.0420 - mean_squared_error: 0.0340 - val_loss: 0.0225 - val_mean_squared_error: 0.0145\n",
      "Epoch 00009: early stopping\n",
      "Weights loaded: best_features_extraction_rbf.h5\n",
      "Weights loaded: best_weights_rbf.h5\n",
      "Original dataset shape: (39644, 60), Labels: (39644, 1)\n",
      "Data is normalize\n",
      "Outlier removed: 0\n",
      "Dataset shape: (39644, 39), Labels: (39644, 1)\n",
      "x_train: (31715, 39), y_train: (31715, 1)\n",
      "x_test: (7929, 39), y_test: (7929, 1)\n",
      "Train on 12686 samples, validate on 7929 samples\n",
      "Epoch 1/50\n",
      "12686/12686 [==============================] - 2s 121us/step - loss: 0.0422 - mean_squared_error: 0.0343 - val_loss: 0.0227 - val_mean_squared_error: 0.0147\n",
      "Epoch 2/50\n",
      "12686/12686 [==============================] - 1s 104us/step - loss: 0.0432 - mean_squared_error: 0.0352 - val_loss: 0.0229 - val_mean_squared_error: 0.0149\n",
      "Epoch 3/50\n",
      "12686/12686 [==============================] - 1s 105us/step - loss: 0.0425 - mean_squared_error: 0.0346 - val_loss: 0.0226 - val_mean_squared_error: 0.0147\n",
      "Epoch 4/50\n",
      "12686/12686 [==============================] - 1s 102us/step - loss: 0.0435 - mean_squared_error: 0.0355 - val_loss: 0.0212 - val_mean_squared_error: 0.0133\n",
      "Epoch 5/50\n",
      "12686/12686 [==============================] - 1s 108us/step - loss: 0.0427 - mean_squared_error: 0.0347 - val_loss: 0.0213 - val_mean_squared_error: 0.0133\n",
      "Epoch 6/50\n",
      "12686/12686 [==============================] - 1s 107us/step - loss: 0.0418 - mean_squared_error: 0.0338 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12686/12686 [==============================] - 1s 101us/step - loss: 0.0426 - mean_squared_error: 0.0347 - val_loss: 0.0234 - val_mean_squared_error: 0.0155\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/50\n",
      "12686/12686 [==============================] - 1s 106us/step - loss: 0.0426 - mean_squared_error: 0.0346 - val_loss: 0.0214 - val_mean_squared_error: 0.0135\n",
      "Epoch 9/50\n",
      "12686/12686 [==============================] - 1s 105us/step - loss: 0.0420 - mean_squared_error: 0.0341 - val_loss: 0.0220 - val_mean_squared_error: 0.0141\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/50\n",
      "12686/12686 [==============================] - 1s 115us/step - loss: 0.0416 - mean_squared_error: 0.0336 - val_loss: 0.0219 - val_mean_squared_error: 0.0139\n",
      "Epoch 00010: early stopping\n",
      "Weights loaded: best_features_extraction_rbf.h5\n",
      "Weights loaded: best_weights_rbf.h5\n",
      "Original dataset shape: (39644, 60), Labels: (39644, 1)\n",
      "Data is normalize\n",
      "Outlier removed: 0\n",
      "Dataset shape: (39644, 39), Labels: (39644, 1)\n",
      "x_train: (31715, 39), y_train: (31715, 1)\n",
      "x_test: (7929, 39), y_test: (7929, 1)\n",
      "Train on 15857 samples, validate on 7929 samples\n",
      "Epoch 1/50\n",
      "15857/15857 [==============================] - 2s 110us/step - loss: 0.0419 - mean_squared_error: 0.0340 - val_loss: 0.0214 - val_mean_squared_error: 0.0135\n",
      "Epoch 2/50\n",
      "15857/15857 [==============================] - 2s 99us/step - loss: 0.0423 - mean_squared_error: 0.0344 - val_loss: 0.0245 - val_mean_squared_error: 0.0165\n",
      "Epoch 3/50\n",
      "15857/15857 [==============================] - 2s 99us/step - loss: 0.0414 - mean_squared_error: 0.0334 - val_loss: 0.0219 - val_mean_squared_error: 0.0139\n",
      "Epoch 4/50\n",
      "15857/15857 [==============================] - 1s 94us/step - loss: 0.0415 - mean_squared_error: 0.0335 - val_loss: 0.0240 - val_mean_squared_error: 0.0160\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 5/50\n",
      "15857/15857 [==============================] - 2s 95us/step - loss: 0.0421 - mean_squared_error: 0.0341 - val_loss: 0.0227 - val_mean_squared_error: 0.0147\n",
      "Epoch 6/50\n",
      "15857/15857 [==============================] - 2s 98us/step - loss: 0.0424 - mean_squared_error: 0.0345 - val_loss: 0.0221 - val_mean_squared_error: 0.0141\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 7/50\n",
      "15857/15857 [==============================] - 1s 93us/step - loss: 0.0416 - mean_squared_error: 0.0336 - val_loss: 0.0216 - val_mean_squared_error: 0.0136\n",
      "Epoch 00007: early stopping\n",
      "Weights loaded: best_features_extraction_rbf.h5\n",
      "Weights loaded: best_weights_rbf.h5\n",
      "Original dataset shape: (39644, 60), Labels: (39644, 1)\n",
      "Data is normalize\n",
      "Outlier removed: 0\n",
      "Dataset shape: (39644, 39), Labels: (39644, 1)\n",
      "x_train: (31715, 39), y_train: (31715, 1)\n",
      "x_test: (7929, 39), y_test: (7929, 1)\n",
      "Train on 19029 samples, validate on 7929 samples\n",
      "Epoch 1/50\n",
      "19029/19029 [==============================] - 2s 105us/step - loss: 0.0412 - mean_squared_error: 0.0333 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n",
      "Epoch 2/50\n",
      "19029/19029 [==============================] - 2s 94us/step - loss: 0.0417 - mean_squared_error: 0.0337 - val_loss: 0.0249 - val_mean_squared_error: 0.0169\n",
      "Epoch 3/50\n",
      "19029/19029 [==============================] - 2s 99us/step - loss: 0.0425 - mean_squared_error: 0.0346 - val_loss: 0.0234 - val_mean_squared_error: 0.0154\n",
      "Epoch 4/50\n",
      "19029/19029 [==============================] - 2s 95us/step - loss: 0.0413 - mean_squared_error: 0.0333 - val_loss: 0.0211 - val_mean_squared_error: 0.0132\n",
      "Epoch 5/50\n",
      "19029/19029 [==============================] - 2s 97us/step - loss: 0.0412 - mean_squared_error: 0.0333 - val_loss: 0.0212 - val_mean_squared_error: 0.0133\n",
      "Epoch 6/50\n",
      "19029/19029 [==============================] - 2s 93us/step - loss: 0.0424 - mean_squared_error: 0.0344 - val_loss: 0.0247 - val_mean_squared_error: 0.0167\n",
      "Epoch 7/50\n",
      "19029/19029 [==============================] - 2s 99us/step - loss: 0.0411 - mean_squared_error: 0.0332 - val_loss: 0.0232 - val_mean_squared_error: 0.0152\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/50\n",
      "19029/19029 [==============================] - 2s 104us/step - loss: 0.0413 - mean_squared_error: 0.0333 - val_loss: 0.0222 - val_mean_squared_error: 0.0143\n",
      "Epoch 9/50\n",
      "19029/19029 [==============================] - 2s 94us/step - loss: 0.0414 - mean_squared_error: 0.0335 - val_loss: 0.0216 - val_mean_squared_error: 0.0136\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/50\n",
      "19029/19029 [==============================] - 2s 101us/step - loss: 0.0406 - mean_squared_error: 0.0327 - val_loss: 0.0214 - val_mean_squared_error: 0.0134\n",
      "Epoch 00010: early stopping\n",
      "Weights loaded: best_features_extraction_rbf.h5\n",
      "Weights loaded: best_weights_rbf.h5\n",
      "Original dataset shape: (39644, 60), Labels: (39644, 1)\n",
      "Data is normalize\n",
      "Outlier removed: 0\n",
      "Dataset shape: (39644, 39), Labels: (39644, 1)\n",
      "x_train: (31715, 39), y_train: (31715, 1)\n",
      "x_test: (7929, 39), y_test: (7929, 1)\n",
      "Train on 22200 samples, validate on 7929 samples\n",
      "Epoch 1/50\n",
      "22200/22200 [==============================] - 2s 108us/step - loss: 0.0411 - mean_squared_error: 0.0332 - val_loss: 0.0228 - val_mean_squared_error: 0.0148\n",
      "Epoch 2/50\n",
      "22200/22200 [==============================] - 2s 94us/step - loss: 0.0419 - mean_squared_error: 0.0339 - val_loss: 0.0221 - val_mean_squared_error: 0.0141\n",
      "Epoch 3/50\n",
      "22200/22200 [==============================] - 2s 93us/step - loss: 0.0415 - mean_squared_error: 0.0336 - val_loss: 0.0215 - val_mean_squared_error: 0.0135\n",
      "Epoch 4/50\n",
      "22200/22200 [==============================] - 2s 98us/step - loss: 0.0411 - mean_squared_error: 0.0332 - val_loss: 0.0236 - val_mean_squared_error: 0.0157\n",
      "Epoch 5/50\n",
      "22200/22200 [==============================] - 2s 97us/step - loss: 0.0410 - mean_squared_error: 0.0331 - val_loss: 0.0211 - val_mean_squared_error: 0.0131\n",
      "Epoch 6/50\n",
      "22200/22200 [==============================] - 2s 101us/step - loss: 0.0412 - mean_squared_error: 0.0332 - val_loss: 0.0227 - val_mean_squared_error: 0.0148\n",
      "Epoch 7/50\n",
      "22200/22200 [==============================] - 2s 95us/step - loss: 0.0405 - mean_squared_error: 0.0326 - val_loss: 0.0210 - val_mean_squared_error: 0.0130\n",
      "Epoch 8/50\n",
      "22200/22200 [==============================] - 2s 94us/step - loss: 0.0414 - mean_squared_error: 0.0335 - val_loss: 0.0209 - val_mean_squared_error: 0.0130\n",
      "Epoch 9/50\n",
      "22200/22200 [==============================] - 2s 95us/step - loss: 0.0416 - mean_squared_error: 0.0336 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 10/50\n",
      "22200/22200 [==============================] - 2s 97us/step - loss: 0.0412 - mean_squared_error: 0.0333 - val_loss: 0.0210 - val_mean_squared_error: 0.0130\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 11/50\n",
      "22200/22200 [==============================] - 2s 98us/step - loss: 0.0414 - mean_squared_error: 0.0334 - val_loss: 0.0222 - val_mean_squared_error: 0.0142\n",
      "Epoch 12/50\n",
      "22200/22200 [==============================] - 2s 99us/step - loss: 0.0410 - mean_squared_error: 0.0330 - val_loss: 0.0221 - val_mean_squared_error: 0.0141\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 13/50\n",
      "22200/22200 [==============================] - 2s 100us/step - loss: 0.0409 - mean_squared_error: 0.0329 - val_loss: 0.0217 - val_mean_squared_error: 0.0137\n",
      "Epoch 00013: early stopping\n",
      "Weights loaded: best_features_extraction_rbf.h5\n",
      "Weights loaded: best_weights_rbf.h5\n",
      "Original dataset shape: (39644, 60), Labels: (39644, 1)\n",
      "Data is normalize\n",
      "Outlier removed: 0\n",
      "Dataset shape: (39644, 39), Labels: (39644, 1)\n",
      "x_train: (31715, 39), y_train: (31715, 1)\n",
      "x_test: (7929, 39), y_test: (7929, 1)\n",
      "Train on 25372 samples, validate on 7929 samples\n",
      "Epoch 1/50\n",
      "25372/25372 [==============================] - 3s 107us/step - loss: 0.0410 - mean_squared_error: 0.0330 - val_loss: 0.0230 - val_mean_squared_error: 0.0150\n",
      "Epoch 2/50\n",
      "25372/25372 [==============================] - 2s 99us/step - loss: 0.0408 - mean_squared_error: 0.0328 - val_loss: 0.0214 - val_mean_squared_error: 0.0135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "25372/25372 [==============================] - 2s 96us/step - loss: 0.0415 - mean_squared_error: 0.0335 - val_loss: 0.0221 - val_mean_squared_error: 0.0141\n",
      "Epoch 4/50\n",
      "25372/25372 [==============================] - 2s 98us/step - loss: 0.0411 - mean_squared_error: 0.0331 - val_loss: 0.0212 - val_mean_squared_error: 0.0132\n",
      "Epoch 5/50\n",
      "25372/25372 [==============================] - 2s 94us/step - loss: 0.0406 - mean_squared_error: 0.0326 - val_loss: 0.0215 - val_mean_squared_error: 0.0135\n",
      "Epoch 6/50\n",
      "25372/25372 [==============================] - 2s 92us/step - loss: 0.0407 - mean_squared_error: 0.0327 - val_loss: 0.0214 - val_mean_squared_error: 0.0134\n",
      "Epoch 7/50\n",
      "25372/25372 [==============================] - 2s 96us/step - loss: 0.0407 - mean_squared_error: 0.0328 - val_loss: 0.0217 - val_mean_squared_error: 0.0138\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/50\n",
      "25372/25372 [==============================] - 2s 96us/step - loss: 0.0408 - mean_squared_error: 0.0328 - val_loss: 0.0214 - val_mean_squared_error: 0.0134\n",
      "Epoch 9/50\n",
      "25372/25372 [==============================] - 2s 96us/step - loss: 0.0414 - mean_squared_error: 0.0334 - val_loss: 0.0224 - val_mean_squared_error: 0.0145\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/50\n",
      "25372/25372 [==============================] - 2s 96us/step - loss: 0.0402 - mean_squared_error: 0.0322 - val_loss: 0.0219 - val_mean_squared_error: 0.0139\n",
      "Epoch 00010: early stopping\n",
      "Weights loaded: best_features_extraction_rbf.h5\n",
      "Weights loaded: best_weights_rbf.h5\n",
      "Original dataset shape: (39644, 60), Labels: (39644, 1)\n",
      "Data is normalize\n",
      "Outlier removed: 0\n",
      "Dataset shape: (39644, 39), Labels: (39644, 1)\n",
      "x_train: (31715, 39), y_train: (31715, 1)\n",
      "x_test: (7929, 39), y_test: (7929, 1)\n",
      "Train on 28543 samples, validate on 7929 samples\n",
      "Epoch 1/50\n",
      "28543/28543 [==============================] - 3s 100us/step - loss: 0.0410 - mean_squared_error: 0.0330 - val_loss: 0.0233 - val_mean_squared_error: 0.0153\n",
      "Epoch 2/50\n",
      "28543/28543 [==============================] - 3s 95us/step - loss: 0.0407 - mean_squared_error: 0.0328 - val_loss: 0.0230 - val_mean_squared_error: 0.0150\n",
      "Epoch 3/50\n",
      "28543/28543 [==============================] - 3s 91us/step - loss: 0.0407 - mean_squared_error: 0.0328 - val_loss: 0.0214 - val_mean_squared_error: 0.0134\n",
      "Epoch 4/50\n",
      "28543/28543 [==============================] - 3s 96us/step - loss: 0.0403 - mean_squared_error: 0.0323 - val_loss: 0.0212 - val_mean_squared_error: 0.0132\n",
      "Epoch 5/50\n",
      "28543/28543 [==============================] - 3s 91us/step - loss: 0.0407 - mean_squared_error: 0.0327 - val_loss: 0.0218 - val_mean_squared_error: 0.0139\n",
      "Epoch 6/50\n",
      "28543/28543 [==============================] - 3s 91us/step - loss: 0.0397 - mean_squared_error: 0.0317 - val_loss: 0.0221 - val_mean_squared_error: 0.0141\n",
      "Epoch 7/50\n",
      "28543/28543 [==============================] - 3s 95us/step - loss: 0.0399 - mean_squared_error: 0.0319 - val_loss: 0.0221 - val_mean_squared_error: 0.0141\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 8/50\n",
      "28543/28543 [==============================] - 3s 97us/step - loss: 0.0398 - mean_squared_error: 0.0318 - val_loss: 0.0216 - val_mean_squared_error: 0.0137\n",
      "Epoch 9/50\n",
      "28543/28543 [==============================] - 3s 94us/step - loss: 0.0405 - mean_squared_error: 0.0326 - val_loss: 0.0225 - val_mean_squared_error: 0.0146\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 10/50\n",
      "28543/28543 [==============================] - 3s 92us/step - loss: 0.0402 - mean_squared_error: 0.0322 - val_loss: 0.0220 - val_mean_squared_error: 0.0141\n",
      "Epoch 00010: early stopping\n",
      "Weights loaded: best_features_extraction_rbf.h5\n",
      "Weights loaded: best_weights_rbf.h5\n",
      "Original dataset shape: (39644, 60), Labels: (39644, 1)\n",
      "Data is normalize\n",
      "Outlier removed: 0\n",
      "Dataset shape: (39644, 39), Labels: (39644, 1)\n",
      "x_train: (31715, 39), y_train: (31715, 1)\n",
      "x_test: (7929, 39), y_test: (7929, 1)\n",
      "Train on 31715 samples, validate on 7929 samples\n",
      "Epoch 1/50\n",
      "31715/31715 [==============================] - 3s 104us/step - loss: 0.0395 - mean_squared_error: 0.0315 - val_loss: 0.0230 - val_mean_squared_error: 0.0150\n",
      "Epoch 2/50\n",
      "31715/31715 [==============================] - 3s 93us/step - loss: 0.0403 - mean_squared_error: 0.0323 - val_loss: 0.0233 - val_mean_squared_error: 0.0153\n",
      "Epoch 3/50\n",
      "31715/31715 [==============================] - 3s 94us/step - loss: 0.0400 - mean_squared_error: 0.0321 - val_loss: 0.0215 - val_mean_squared_error: 0.0136\n",
      "Epoch 4/50\n",
      "31715/31715 [==============================] - 3s 93us/step - loss: 0.0400 - mean_squared_error: 0.0320 - val_loss: 0.0246 - val_mean_squared_error: 0.0166\n",
      "Epoch 5/50\n",
      "31715/31715 [==============================] - 3s 98us/step - loss: 0.0398 - mean_squared_error: 0.0319 - val_loss: 0.0229 - val_mean_squared_error: 0.0149\n",
      "Epoch 6/50\n",
      "31715/31715 [==============================] - 3s 101us/step - loss: 0.0408 - mean_squared_error: 0.0329 - val_loss: 0.0219 - val_mean_squared_error: 0.0139\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 7/50\n",
      "31715/31715 [==============================] - 3s 100us/step - loss: 0.0395 - mean_squared_error: 0.0315 - val_loss: 0.0233 - val_mean_squared_error: 0.0153\n",
      "Epoch 8/50\n",
      "31715/31715 [==============================] - 3s 105us/step - loss: 0.0397 - mean_squared_error: 0.0317 - val_loss: 0.0222 - val_mean_squared_error: 0.0143\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 9/50\n",
      "31715/31715 [==============================] - 3s 109us/step - loss: 0.0392 - mean_squared_error: 0.0312 - val_loss: 0.0218 - val_mean_squared_error: 0.0138\n",
      "Epoch 00009: early stopping\n"
     ]
    }
   ],
   "source": [
    "histories = []\n",
    "for i in range(1, 11):\n",
    "    l2_kernal = 0.0001\n",
    "\n",
    "    # load pretrained MLP weights\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=39, activation='relu', kernel_regularizer=regularizers.l2(l2_kernal)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(20, activation='relu', kernel_regularizer=regularizers.l2(l2_kernal)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid', activity_regularizer=regularizers.l2(l2_kernal)))\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    saved_weights_name = 'best_features_extraction_rbf.h5'\n",
    "    if os.path.isfile(saved_weights_name):\n",
    "        model.load_weights(saved_weights_name)\n",
    "        print(\"Weights loaded: {0}\".format(saved_weights_name))\n",
    "\n",
    "    # modify extraction network final layer\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.layers.pop()\n",
    "    model.outputs = [model.layers[-1].output]\n",
    "\n",
    "    # Add RBL layer to model\n",
    "    rbflayer = RBFLayer(20,\n",
    "                        betas=0.1,\n",
    "                        input_shape=(20,))\n",
    "    model.add(rbflayer)\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=Adam(lr=0.001),\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    # training with callbacks\n",
    "    saved_weights_name = 'best_weights_rbf.h5'\n",
    "\n",
    "    if os.path.isfile(saved_weights_name):\n",
    "        model.load_weights(saved_weights_name)\n",
    "        print(\"Weights loaded: {0}\".format(saved_weights_name))\n",
    "\n",
    "    # kfold on dataset and training\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    dataset, labels = load_dataset(is_normalize=True)\n",
    "    print('Dataset shape: {0}, Labels: {1}'.format(dataset.shape, labels.shape))\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, shuffle=False, random_state=42)\n",
    "    print(\"x_train: {0}, y_train: {1}\".format(x_train.shape, y_train.shape))\n",
    "    print(\"x_test: {0}, y_test: {1}\".format(x_test.shape, y_test.shape))\n",
    "\n",
    "    sub_x_train = x_train[0: int(len(x_train) * (i / 10))]\n",
    "    sub_y_train = y_train[0: int(len(y_train) * (i / 10))]\n",
    "    \n",
    "    # train model\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0001,\n",
    "                               patience=10,\n",
    "                               mode='min',\n",
    "                               verbose=1)\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                  factor=0.2,\n",
    "                                  patience=2,\n",
    "                                  min_lr=0.00001,\n",
    "                                  verbose=1)\n",
    "\n",
    "    history = model.fit(sub_x_train, sub_y_train,\n",
    "                          epochs=50,\n",
    "                          validation_data=(x_test, y_test),\n",
    "                          callbacks=[early_stop, reduce_lr],\n",
    "                          verbose=1)\n",
    "    \n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1765c4c6b70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17488930860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size_percentage = list(range(1, 11))\n",
    "histories_loss = []\n",
    "histories_mse = []\n",
    "for h in histories:\n",
    "    histories_loss.append(h.history['loss'][-1])\n",
    "    histories_mse.append(h.history['val_mean_squared_error'][-1])\n",
    "    \n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(size_percentage, histories_loss, label='loss score')\n",
    "plt.plot(size_percentage, histories_mse, label='validation score')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('mse')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
