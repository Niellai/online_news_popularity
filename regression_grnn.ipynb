{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neupy import algorithms, estimators, environment\n",
    "import csv\n",
    "import os.path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "from rbflayer import RBFLayer, InitCentersRandom\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import *\n",
    "\n",
    "environment.reproducible()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading of dataset \n",
    "dataset = []\n",
    "labels = []\n",
    "headers = []\n",
    "\n",
    "def load_dataset_raw():\n",
    "    global dataset, labels, headers\n",
    "    with open('OnlineNewsPopularity.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        headers = next(reader) # the first row\n",
    "        headers = headers[1:]\n",
    "        headers = [x.strip(' ') for x in headers]    \n",
    "            \n",
    "        temp_data = []\n",
    "        temp_label = []\n",
    "        for idx, row in enumerate(reader):\n",
    "            temp_data.append(row[1:])\n",
    "            temp_label.append(row[-1])\n",
    "\n",
    "        # convert elements to float        \n",
    "        for idx, row in enumerate(temp_data):        \n",
    "            dataset.append([float(i) for i in row[:]])\n",
    "        for idx, row in enumerate(temp_label):\n",
    "            labels.append(float(row))   \n",
    "        \n",
    "    return np.array(dataset), np.array(labels).reshape(-1, 1)\n",
    "\n",
    "def load_dataset(is_normalize=True, is_remove_outlier=True, select_top=50):\n",
    "    \"\"\"\n",
    "    is_normalize: To normalize or keep orignal form\n",
    "    is_remove_outlier: To remove outlier \n",
    "    select_top: select top important features using PCA (Principal component analysis)\n",
    "    \"\"\"\n",
    "    global dataset, labels, headers\n",
    "    dataset, labels = load_dataset_raw()\n",
    "    print('Original dataset shape: {0}, Labels: {1}'.format(dataset.shape, labels.shape))\n",
    "    \n",
    "    fields_to_avoid = [0, 18, 20, 24, 38, 39, 40, 41, 42]\n",
    "    weekday = [30, 31, 32, 33, 34, 35, 36, 37]\n",
    "    data_channel = [12, 13, 14, 15, 16, 17]\n",
    "    fields_to_avoid = fields_to_avoid + weekday + data_channel\n",
    "    fields_to_avoid = sorted(fields_to_avoid)\n",
    "\n",
    "    # adjust headers\n",
    "    sub_headers = []\n",
    "    for idx, h in enumerate(headers):\n",
    "        if idx not in fields_to_avoid:\n",
    "            sub_headers.append(h)\n",
    "    sub_headers.insert(len(sub_headers) - 1, 'weekday')\n",
    "    sub_headers.insert(len(sub_headers) - 1, 'data_channel')\n",
    "    headers = sub_headers\n",
    "    \n",
    "    # filter columns and categorical fields\n",
    "    new_dataset = []        \n",
    "    for idx, row in enumerate(dataset):\n",
    "        slice_row = [i for num,i in enumerate(row) if num not in fields_to_avoid]\n",
    "        insert_idx = len(slice_row) - 1\n",
    "\n",
    "        for col_idx in range(len(row)):\n",
    "            # convert week day to categorical \n",
    "            if col_idx in weekday and row[col_idx] == 1:\n",
    "                if col_idx == 30:\n",
    "                    slice_row.insert(insert_idx, 1)\n",
    "                elif col_idx == 31:\n",
    "                    slice_row.insert(insert_idx, 2)\n",
    "                elif col_idx == 32:\n",
    "                    slice_row.insert(insert_idx, 3)\n",
    "                elif col_idx == 33:\n",
    "                    slice_row.insert(insert_idx, 4)\n",
    "                elif col_idx == 34:\n",
    "                    slice_row.insert(insert_idx, 5)\n",
    "                elif col_idx == 35:\n",
    "                    slice_row.insert(insert_idx, 6)\n",
    "                elif col_idx == 36:\n",
    "                    slice_row.insert(insert_idx, 7)\n",
    "\n",
    "            # convert data channel to categorical \n",
    "            if col_idx in data_channel and row[col_idx] == 1:\n",
    "                if col_idx == 12:\n",
    "                    slice_row.insert(insert_idx, 1)\n",
    "                elif col_idx == 13:\n",
    "                    slice_row.insert(insert_idx, 2)\n",
    "                elif col_idx == 14:\n",
    "                    slice_row.insert(insert_idx, 3)\n",
    "                elif col_idx == 15:\n",
    "                    slice_row.insert(insert_idx, 4)\n",
    "                elif col_idx == 16:\n",
    "                    slice_row.insert(insert_idx, 5)\n",
    "                elif col_idx == 17:\n",
    "                    slice_row.insert(insert_idx, 6)\n",
    "\n",
    "        # handle missing data\n",
    "        if len(slice_row) == 38:\n",
    "            slice_row.append(0)\n",
    "        new_dataset.append(slice_row)         \n",
    "    \n",
    "    # copy filter new_dataset to dataset \n",
    "    dataset = new_dataset\n",
    "    \n",
    "    # normalizing in dataset\n",
    "    if is_normalize:\n",
    "        # it is important which type of normalization method you use\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        dataset = scaler.fit_transform(dataset)\n",
    "        # last item uses MinMaxScaler\n",
    "        labels = np.array(labels).reshape(-1, 1)\n",
    "        scaler = preprocessing.QuantileTransformer()\n",
    "        labels = scaler.fit_transform(labels)\n",
    "        print('Data is normalize')\n",
    "    else:\n",
    "        print('No normalize apply')        \n",
    "\n",
    "    # remove outlier in dataset\n",
    "    if is_remove_outlier:\n",
    "        dataset, labels = reject_outliers(dataset, labels)    \n",
    "\n",
    "    # applying PCA in dataset\n",
    "    if 0 < select_top < len(dataset[0]):\n",
    "        top_features = pca_important_features(dataset, headers, top=select_top)\n",
    "        feature_keys = [f[1] for f in top_features]\n",
    "        df = pd.DataFrame(dataset, columns=headers)\n",
    "        top_dataset = []\n",
    "        for name in feature_keys:\n",
    "            top_dataset.append(df.loc[:, name])\n",
    "        dataset = np.transpose(top_dataset)\n",
    "        \n",
    "    return np.array(dataset), np.array(labels).reshape(-1, 1)\n",
    "\n",
    "def pca_important_features(dataset, headers, top=10):\n",
    "    scaleFeatures = True\n",
    "    pca = PCA(n_components=top)\n",
    "    projected  = pca.fit_transform(dataset)\n",
    "    top_features = extract_features(projected, pca.components_, headers, top)\n",
    "    return top_features\n",
    "    \n",
    "def extract_features(transformed_features, components_, columns, top):\n",
    "    num_columns = len(columns)\n",
    "\n",
    "    # This funtion will project your *original* feature (columns)\n",
    "    # onto your principal component feature-space, so that you can\n",
    "    # visualize how \"important\" each one was in the\n",
    "    # multi-dimensional scaling\n",
    "\n",
    "    # Scale the principal components by the max value in\n",
    "    # the transformed set belonging to that component\n",
    "    xvector = components_[0] * max(transformed_features[:,0])\n",
    "    yvector = components_[1] * max(transformed_features[:,1])\n",
    "\n",
    "    # Sort each column by it's length. These are your *original*\n",
    "    # columns, not the principal components.\n",
    "    important_features = { columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(num_columns) }\n",
    "    important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True)\n",
    "    print(\"Features by top {0} importance:\".format(top))\n",
    "    for feature in important_features[:top]:\n",
    "        print(feature)\n",
    "    print()\n",
    "    return important_features[:top]\n",
    "        \n",
    "def reject_outliers(dataset, labels, m=50):        \n",
    "    \"\"\"\n",
    "    Higher the value 'm' more outliers\n",
    "    \"\"\"\n",
    "    d = np.abs(labels - np.median(labels))\n",
    "    mdev = np.median(d)\n",
    "    s = d/mdev if mdev else 0.\n",
    "        \n",
    "    # remove outliers in dataset and labels\n",
    "    sub_dataset = []\n",
    "    sub_labels = []\n",
    "    outlier_labels = []\n",
    "    for idx, item in enumerate(s):        \n",
    "        if item < m:\n",
    "            sub_labels.append(labels[idx])\n",
    "            sub_dataset.append(dataset[idx])        \n",
    "        else:\n",
    "            outlier_labels.append(labels[idx])\n",
    "    print('Outlier removed: {0}'.format(len(outlier_labels)))\n",
    "    return sub_dataset, sub_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (39644, 60), Labels: (39644, 1)\n",
      "Data is normalize\n",
      "Outlier removed: 0\n",
      "Dataset shape: (39644, 39), Labels: (39644, 1)\n",
      "x_train: (31715, 39), y_train: (31715, 1)\n",
      "x_test: (7929, 39), y_test: (7929, 1)\n"
     ]
    }
   ],
   "source": [
    "# Splitting of dataset into train and test\n",
    "dataset = []\n",
    "labels = []\n",
    "dataset, labels = load_dataset(is_normalize=True, is_remove_outlier=True, select_top=-1)\n",
    "print('Dataset shape: {0}, Labels: {1}'.format(dataset.shape, labels.shape))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, shuffle=False, random_state=42)\n",
    "print(\"x_train: {0}, y_train: {1}\".format(x_train.shape, y_train.shape))\n",
    "print(\"x_test: {0}, y_test: {1}\".format(x_test.shape, y_test.shape))\n",
    "\n",
    "# print(dataset[0])\n",
    "# print(labels[:-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Main information\n",
      "\n",
      "[ALGORITHM] GRNN\n",
      "\r\n",
      "[OPTION] verbose = True\n",
      "[OPTION] epoch_end_signal = None\n",
      "[OPTION] show_epoch = 1\n",
      "[OPTION] shuffle_data = False\n",
      "[OPTION] step = 0.1\n",
      "[OPTION] train_end_signal = None\n",
      "[OPTION] std = 0.1\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "grnn = algorithms.GRNN(std=0.1, verbose=True)\n",
    "grnn.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3294559214762544"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = grnn.predict(x_test)\n",
    "estimators.rmse(y_predicted, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
